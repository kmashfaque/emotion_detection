{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b880b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b603704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen=ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9287c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator=train_data_gen.flow_from_directory(\"C:/Users/HP/Desktop/AI/images/train/\",\n",
    "target_size=(48,48),batch_size=64,color_mode=\"grayscale\",class_mode=\"categorical\")\n",
    "\n",
    "validation_generator=train_data_gen.flow_from_directory(\"C:/Users/HP/Desktop/AI/images/validation/\",\n",
    "target_size=(48,48),batch_size=64,color_mode=\"grayscale\",class_mode=\"categorical\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84fb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3b4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(32,kernel_size=(3,3),activation=\"relu\",input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64,kernel_size=(3,3),activation=\"relu\"))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef7d0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "100b864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.compile(loss=\"categorical_crossentropy\",optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b088a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10/10 [==============================] - 52s 5s/step - loss: 1.8900 - accuracy: 0.2219 - val_loss: 1.8642 - val_accuracy: 0.2656\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 39s 4s/step - loss: 1.8341 - accuracy: 0.2656 - val_loss: 1.8252 - val_accuracy: 0.2859\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 35s 4s/step - loss: 1.8559 - accuracy: 0.2188 - val_loss: 1.8374 - val_accuracy: 0.2531\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 32s 3s/step - loss: 1.8264 - accuracy: 0.2250 - val_loss: 1.8355 - val_accuracy: 0.2406\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 30s 3s/step - loss: 1.8219 - accuracy: 0.2516 - val_loss: 1.8342 - val_accuracy: 0.2484\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.8105 - accuracy: 0.2734 - val_loss: 1.8227 - val_accuracy: 0.2656\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 26s 3s/step - loss: 1.8340 - accuracy: 0.2219 - val_loss: 1.8335 - val_accuracy: 0.2484\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 25s 3s/step - loss: 1.8241 - accuracy: 0.2391 - val_loss: 1.8480 - val_accuracy: 0.2516\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.8440 - accuracy: 0.2391 - val_loss: 1.8435 - val_accuracy: 0.2359\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.8408 - accuracy: 0.2500 - val_loss: 1.8451 - val_accuracy: 0.2484\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 20s 2s/step - loss: 1.8017 - accuracy: 0.2609 - val_loss: 1.8423 - val_accuracy: 0.2391\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 19s 2s/step - loss: 1.8113 - accuracy: 0.2500 - val_loss: 1.8243 - val_accuracy: 0.2812\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 18s 2s/step - loss: 1.8325 - accuracy: 0.2379 - val_loss: 1.8257 - val_accuracy: 0.2781\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 18s 2s/step - loss: 1.8084 - accuracy: 0.2484 - val_loss: 1.8227 - val_accuracy: 0.2672\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.8228 - accuracy: 0.2641 - val_loss: 1.8184 - val_accuracy: 0.2656\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.8450 - accuracy: 0.2141 - val_loss: 1.8466 - val_accuracy: 0.2391\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.8144 - accuracy: 0.2469 - val_loss: 1.8399 - val_accuracy: 0.2500\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 18s 2s/step - loss: 1.8330 - accuracy: 0.2250 - val_loss: 1.8249 - val_accuracy: 0.2547\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 17s 2s/step - loss: 1.8121 - accuracy: 0.2531 - val_loss: 1.8295 - val_accuracy: 0.2547\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 16s 2s/step - loss: 1.8205 - accuracy: 0.2266 - val_loss: 1.8184 - val_accuracy: 0.2828\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 23s 2s/step - loss: 1.8180 - accuracy: 0.2297 - val_loss: 1.8253 - val_accuracy: 0.2438\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 19s 2s/step - loss: 1.8223 - accuracy: 0.2578 - val_loss: 1.8133 - val_accuracy: 0.2750\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 25s 2s/step - loss: 1.8546 - accuracy: 0.2141 - val_loss: 1.8359 - val_accuracy: 0.2500\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 18s 2s/step - loss: 1.8098 - accuracy: 0.2484 - val_loss: 1.8182 - val_accuracy: 0.2859\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 15s 2s/step - loss: 1.8345 - accuracy: 0.2453 - val_loss: 1.8261 - val_accuracy: 0.2469\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 15s 1s/step - loss: 1.8181 - accuracy: 0.2531 - val_loss: 1.8071 - val_accuracy: 0.2922\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 14s 1s/step - loss: 1.8031 - accuracy: 0.2328 - val_loss: 1.8145 - val_accuracy: 0.2781\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 24s 2s/step - loss: 1.8060 - accuracy: 0.2406 - val_loss: 1.8201 - val_accuracy: 0.2859\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 18s 2s/step - loss: 1.7953 - accuracy: 0.2594 - val_loss: 1.8032 - val_accuracy: 0.2625\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 14s 1s/step - loss: 1.8003 - accuracy: 0.2656 - val_loss: 1.8050 - val_accuracy: 0.2141\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info=emotion_model.fit_generator(\n",
    "train_generator,steps_per_epoch=10,epochs=30,validation_data=validation_generator,\n",
    "    validation_steps=10)\n",
    "\n",
    "model_json=emotion_model.to_json()\n",
    "\n",
    "with open(\"emotion_model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "emotion_model.save_weights(\"emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "556aa799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model disk\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# load json and create model\n",
    "json_file=open(\"emotion_model.json\",\"r\")\n",
    "loaded_model_json=json_file.read()\n",
    "json_file.close()\n",
    "emotion_model=model_from_json(loaded_model_json)\n",
    "\n",
    "\n",
    "# load weights into new model\n",
    "emotion_model.load_weights(\"emotion_model.h5\")\n",
    "print(\"loaded model disk\")\n",
    "\n",
    "# start the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# cap = cv2.VideoCapture(\"C:\\\\Users\\\\HP\\\\Desktop\\\\AI\\\\CNN\\\\emotion_detection\\\\1987949503.mp4\")\n",
    "\n",
    "while True:\n",
    "    # find haar cscade to draw bounding box around face\n",
    "    ret,frame=cap.read()\n",
    "    frame=cv2.resize(frame,(1280,720))\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector=cv2.CascadeClassifier(\"haarcascades/haarcascade_frontalface_default.xml\")\n",
    "    gray_frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    # detect faces available on camera\n",
    "    num_faces=face_detector.detectMultiScale(gray_frame,scaleFactor=1.3,minNeighbors=5)\n",
    "\n",
    "    # take each face available on the camera and preprocess it\n",
    "    for (x,y,w,h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y - 50), (x + w, y + h + 10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "        # predict the emotions\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x + 5, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2087d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea7143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
